{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_LABELS=7\n",
    "LABELS=[\"airplane\",\n",
    "        \"face\",\n",
    "        \"knife\",\n",
    "        \"motorbike\",\n",
    "        \"other\",\n",
    "        \"pistol\",\n",
    "        \"NA\"]\n",
    "NET_NAME=\"net\"\n",
    "TRAINED_CHECKPOINT=\"gs://alex-s2t-test/adversarial/nets_ckpt/inception_resnet_v2_2016_08_30.ckpt\"\n",
    "TRANSFER_LEARNING_CHECKPOINT=\"gs://alex-s2t-test/adversarial/datasets/six_labels/inception_resnet/training/train/model.ckpt-1000\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "rm -rf resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.slim as slim\n",
    "import tensorflow.contrib.slim.nets as nets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we set up the input image. We use a tf.Variable instead of a tf.placeholder because we will need it to be trainable. We can still feed it when we want to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = tf.Variable(tf.zeros((299, 299, 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building retrained inception resnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from trainer.inception_resnet_v2_builder import InceptionResnetV2Builder as ModelBuilder\n",
    "modelBuilder = ModelBuilder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_body_checkpoint(session, checkpoint_path):\n",
    "    variables_to_exclude_from_scope = ModelBuilder.CHECKPOINT_VARIABLES_TO_EXCLUDE_FROM_SCOPE\n",
    "    all_vars = tf.contrib.slim.get_variables_to_restore(\n",
    "        exclude=variables_to_exclude_from_scope\n",
    "    )\n",
    "\n",
    "    reader = tf.train.NewCheckpointReader(checkpoint_path)\n",
    "    var_to_shape_map = reader.get_variable_to_shape_map()\n",
    "    inception_vars = {\n",
    "        var.op.name: var\n",
    "        for var in all_vars if var.op.name in var_to_shape_map\n",
    "    }\n",
    "\n",
    "    saver = tf.train.Saver(inception_vars)\n",
    "    saver.restore(session, checkpoint_path)\n",
    "    \n",
    "def restore_head_checkpoint(session, checkpoint_path, layer1_weights, layer1_biases, layer2_weights, layer2_biases):\n",
    "    var_list = {\n",
    "        \"InceptionResnetV2/NewLogits/fully_connected/weights\" : layer1_weights,\n",
    "        \"InceptionResnetV2/NewLogits/fully_connected/biases\" : layer1_biases,\n",
    "        \"InceptionResnetV2/NewLogits/fully_connected_1/weights\" : layer2_weights,\n",
    "        \"InceptionResnetV2/NewLogits/fully_connected_1/biases\" : layer2_biases\n",
    "    }\n",
    "    saver = tf.train.Saver(var_list=var_list)\n",
    "    saver.restore(session, checkpoint_path)  \n",
    "\n",
    "def build_head(embeddings, num_labels, scope):\n",
    "    #print num_labels\n",
    "    softmax, endpoints, ordered_endpoints = ModelBuilder().build_predict_model(embeddings, \n",
    "        num_classes=num_labels,\n",
    "        final_endpoint=\"PreLogitsFlatten\",\n",
    "        final_layer_type=\"Softmax\",\n",
    "        reverse=True,\n",
    "        scope=scope)\n",
    "    #print softmax.get_shape().as_list()\n",
    "    with tf.name_scope(scope+\"/prediction\"):\n",
    "        prediction = tf.argmax(softmax, 1)\n",
    "    \n",
    "    weights = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=scope)\n",
    "    for w in weights:\n",
    "        if \"fully_connected/weights\" in w.name:\n",
    "            fc_weights=w\n",
    "        if \"fully_connected/biases\" in w.name:\n",
    "            fc_biases=w\n",
    "        if \"fully_connected_1/weights\" in w.name:\n",
    "            fc1_weights=w\n",
    "        if \"fully_connected_1/biases\" in w.name:\n",
    "            fc1_biases=w\n",
    "    \n",
    "    return endpoints[\"Logits\"], softmax, prediction, fc_weights, fc_biases, fc1_weights, fc1_biases\n",
    "\n",
    "def build_inception_resnet(images, initialize=False):\n",
    "\n",
    "    print(\"building body..\")\n",
    "    embeddings, end_points, ordered_end_points = modelBuilder.build_predict_model(\n",
    "        images, num_classes=NUM_LABELS, final_endpoint='PreLogitsFlatten')\n",
    "    \n",
    "    \n",
    "    weights={}\n",
    "\n",
    "    print(\"building heads...\")\n",
    "    logits, softmax, prediction, fc_weights, fc_biases, fc1_weights, fc1_biases = build_head(\n",
    "        embeddings, NUM_LABELS, NET_NAME\n",
    "    )\n",
    "    \n",
    "    weights={\n",
    "        \"softmax\":softmax,\n",
    "        \"prediction\":prediction,\n",
    "        \"fc_weights\":fc_weights,\n",
    "        \"fc_biases\":fc_biases,\n",
    "        \"fc1_weights\":fc1_weights,\n",
    "        \"fc1_biases\":fc1_biases\n",
    "    }\n",
    "\n",
    "    outputs={}    \n",
    "\n",
    "    if initialize:\n",
    "        print(\"loading body checkpoint...\")\n",
    "        load_body_checkpoint(sess, TRAINED_CHECKPOINT)\n",
    "        print(\"loading heads checkpoints...\")\n",
    "\n",
    "        w = weights\n",
    "        restore_head_checkpoint(\n",
    "            sess, \n",
    "            TRANSFER_LEARNING_CHECKPOINT, \n",
    "            w[\"fc_weights\"],\n",
    "            w[\"fc_biases\"],\n",
    "            w[\"fc1_weights\"],\n",
    "            w[\"fc1_biases\"]\n",
    "        )\n",
    "    outputs[\"prediction\"]=w[\"prediction\"]\n",
    "    outputs[\"scores\"]=w[\"softmax\"]\n",
    "\n",
    "    return logits, outputs[\"scores\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inception(image, reuse=False):\n",
    "    preprocessed = tf.multiply(tf.subtract(tf.expand_dims(image, 0), 0.5), 2.0)\n",
    "    arg_scope = nets.inception.inception_v3_arg_scope(weight_decay=0.0)\n",
    "    with slim.arg_scope(arg_scope):\n",
    "        logits, _ = nets.inception.inception_v3(\n",
    "            preprocessed, 1001, is_training=False, reuse=reuse)\n",
    "        logits = logits[:,1:] # ignore background class\n",
    "        probs = tf.nn.softmax(logits) # probabilities\n",
    "    return logits, probs\n",
    "\n",
    "#logits, probs = inception(image, reuse=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inception_resnet(image, reuse=False, initialize=False):\n",
    "    with tf.variable_scope(\"model\", reuse=reuse):\n",
    "        preprocessed = tf.multiply(tf.subtract(tf.expand_dims(image, 0), 0.5), 2.0)\n",
    "        logits, probs=build_inception_resnet(preprocessed, initialize)\n",
    "    return logits, probs\n",
    "\n",
    "#logits, probs = inception_resnet(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building body..\n",
      "building heads...\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'fc_weights' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-3811de77e413>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minception_resnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitialize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-9-b7c43f6c2e61>\u001b[0m in \u001b[0;36minception_resnet\u001b[0;34m(image, reuse, initialize)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariable_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"model\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreuse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreuse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0mpreprocessed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultiply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubtract\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbuild_inception_resnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreprocessed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitialize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprobs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-3cb65ea1a392>\u001b[0m in \u001b[0;36mbuild_inception_resnet\u001b[0;34m(images, initialize)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"building heads...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     logits, softmax, prediction, fc_weights, fc_biases, fc1_weights, fc1_biases = build_head(\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0membeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNUM_LABELS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNET_NAME\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m     )\n\u001b[1;32m     65\u001b[0m     weights={\n",
      "\u001b[0;32m<ipython-input-7-3cb65ea1a392>\u001b[0m in \u001b[0;36mbuild_head\u001b[0;34m(embeddings, num_labels, scope)\u001b[0m\n\u001b[1;32m     48\u001b[0m             \u001b[0mfc1_biases\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mendpoints\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Logits\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msoftmax\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprediction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfc_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfc_biases\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfc1_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfc1_biases\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mbuild_inception_resnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitialize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'fc_weights' referenced before assignment"
     ]
    }
   ],
   "source": [
    "logits, probs = inception_resnet(image, initialize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imagenet_json=\"resources/imagenet.json\"\n",
    "#with open(imagenet_json) as f:\n",
    "#    imagenet_labels = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(img, correct_class=None, target_class=None):\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 8))\n",
    "    fig.sca(ax1)\n",
    "    p = sess.run(probs, feed_dict={image: img})[0]\n",
    "    ax1.imshow(img)\n",
    "    fig.sca(ax1)\n",
    "    \n",
    "    topk = list(p.argsort()[-NUM_LABELS:][::-1])\n",
    "    print(topk)\n",
    "    topprobs = p[topk]\n",
    "    barlist = ax2.bar(range(NUM_LABELS), topprobs)\n",
    "    if target_class in topk:\n",
    "        barlist[topk.index(target_class)].set_color('r')\n",
    "    if correct_class in topk:\n",
    "        barlist[topk.index(correct_class)].set_color('g')\n",
    "    plt.sca(ax2)\n",
    "    plt.ylim([0, 1.1])\n",
    "    plt.xticks(range(NUM_LABELS),\n",
    "               [LABELS[i] for i in topk],\n",
    "               rotation='vertical')\n",
    "    fig.subplots_adjust(bottom=0.2)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "wget 'http://www.anishathalye.com/media/2017/07/25/cat.jpg' -P resources\n",
    "tar -xf resources/inception_v3_2016_08_28.tar.gz -C resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = 'resources/cat.jpg'\n",
    "img_class = 281\n",
    "img = PIL.Image.open(img_path)\n",
    "big_dim = max(img.width, img.height)\n",
    "wide = img.width > img.height\n",
    "new_w = 299 if not wide else int(img.width * 299 / img.height)\n",
    "new_h = 299 if wide else int(img.height * 299 / img.width)\n",
    "img = img.resize((new_w, new_h)).crop((0, 0, 299, 299))\n",
    "img = (np.asarray(img) / 255.0).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classify(img, correct_class=img_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adversarial examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, (299, 299, 3))\n",
    "\n",
    "x_hat = image # our trainable adversarial input\n",
    "assign_op = tf.assign(x_hat, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = tf.placeholder(tf.float32, ())\n",
    "y_hat = tf.placeholder(tf.int32, ())\n",
    "\n",
    "labels = tf.one_hot(y_hat, NUM_LABELS)\n",
    "loss = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=[labels])\n",
    "optim_step = tf.train.GradientDescentOptimizer(\n",
    "    learning_rate).minimize(loss, var_list=[x_hat])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = tf.placeholder(tf.float32, ())\n",
    "\n",
    "below = x - epsilon\n",
    "above = x + epsilon\n",
    "projected = tf.clip_by_value(tf.clip_by_value(x_hat, below, above), 0, 1)\n",
    "with tf.control_dependencies([projected]):\n",
    "    project_step = tf.assign(x_hat, projected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "demo_epsilon = 2.0/255.0 # a really small perturbation\n",
    "demo_lr = 1e-1\n",
    "demo_steps = 100\n",
    "demo_target = 3 # \"knife\"\n",
    "\n",
    "# initialization step\n",
    "sess.run(assign_op, feed_dict={x: img})\n",
    "\n",
    "# projected gradient descent\n",
    "for i in range(demo_steps):\n",
    "    # gradient descent step\n",
    "    _, loss_value = sess.run(\n",
    "        [optim_step, loss],\n",
    "        feed_dict={learning_rate: demo_lr, y_hat: demo_target})\n",
    "    # project step\n",
    "    sess.run(project_step, feed_dict={x: img, epsilon: demo_epsilon})\n",
    "    if (i+1) % 10 == 0:\n",
    "        print('step %d, loss=%g' % (i+1, loss_value))\n",
    "    \n",
    "\n",
    "adv = x_hat.eval() # retrieve the adversarial example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classify(adv, correct_class=img_class, target_class=demo_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "demo_epsilon = 2.0/255.0 # a really small perturbation\n",
    "demo_lr = 1e-1\n",
    "demo_steps = 1000\n",
    "demo_target = 3 # \"knife\"\n",
    "\n",
    "# initialization step\n",
    "sess.run(assign_op, feed_dict={x: img})\n",
    "\n",
    "# projected gradient descent\n",
    "for i in range(demo_steps):\n",
    "    # gradient descent step\n",
    "    _, loss_value = sess.run(\n",
    "        [optim_step, loss],\n",
    "        feed_dict={learning_rate: demo_lr, y_hat: demo_target})\n",
    "    # project step\n",
    "    sess.run(project_step, feed_dict={x: img, epsilon: demo_epsilon})\n",
    "    if (i+1) % 10 == 0:\n",
    "        print('step %d, loss=%g' % (i+1, loss_value))\n",
    "    \n",
    "\n",
    "adv = x_hat.eval() # retrieve the adversarial example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classify(adv, correct_class=img_class, target_class=demo_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Robust adversarial examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_angle = np.pi/8\n",
    "\n",
    "angle = tf.placeholder(tf.float32, ())\n",
    "rotated_image = tf.contrib.image.rotate(image, angle)\n",
    "rotated_example = rotated_image.eval(feed_dict={image: adv, angle: ex_angle})\n",
    "classify(rotated_example, correct_class=img_class, target_class=demo_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 10\n",
    "average_loss = 0\n",
    "\n",
    "for i in range(num_samples):\n",
    "    rotated = tf.contrib.image.rotate(\n",
    "        image, tf.random_uniform((), minval=-np.pi/4, maxval=np.pi/4))\n",
    "    rotated_logits, _ = inception_resnet(rotated)\n",
    "    average_loss += tf.nn.softmax_cross_entropy_with_logits(\n",
    "        logits=rotated_logits, labels=labels) / num_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 10\n",
    "average_loss = 0\n",
    "for i in range(num_samples):\n",
    "    rotated = tf.contrib.image.rotate(\n",
    "        image, tf.random_uniform((), minval=-np.pi/4, maxval=np.pi/4))\n",
    "    rotated_logits, _ = inception(rotated, reuse=True)\n",
    "    average_loss += tf.nn.softmax_cross_entropy_with_logits(\n",
    "        logits=rotated_logits, labels=labels) / num_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim_step = tf.train.GradientDescentOptimizer(\n",
    "    learning_rate).minimize(average_loss, var_list=[x_hat])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_epsilon = 8.0/255.0 # still a pretty small perturbation\n",
    "demo_lr = 2e-1\n",
    "demo_steps = 300\n",
    "demo_target = 924 # \"guacamole\"\n",
    "\n",
    "# initialization step\n",
    "sess.run(assign_op, feed_dict={x: img})\n",
    "\n",
    "# projected gradient descent\n",
    "for i in range(demo_steps):\n",
    "    # gradient descent step\n",
    "    _, loss_value = sess.run(\n",
    "        [optim_step, average_loss],\n",
    "        feed_dict={learning_rate: demo_lr, y_hat: demo_target})\n",
    "    # project step\n",
    "    sess.run(project_step, feed_dict={x: img, epsilon: demo_epsilon})\n",
    "    if (i+1) % 50 == 0:\n",
    "        print('step %d, loss=%g' % (i+1, loss_value))\n",
    "    \n",
    "\n",
    "adv_robust = x_hat.eval() # retrieve the adversarial example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rotated_example = rotated_image.eval(feed_dict={image: adv_robust, angle: ex_angle})\n",
    "classify(rotated_example, correct_class=img_class, target_class=demo_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thetas = np.linspace(-np.pi/4, np.pi/4, 301)\n",
    "\n",
    "p_naive = []\n",
    "p_robust = []\n",
    "for theta in thetas:\n",
    "    rotated = rotated_image.eval(feed_dict={image: adv_robust, angle: theta})\n",
    "    p_robust.append(probs.eval(feed_dict={image: rotated})[0][demo_target])\n",
    "    \n",
    "    rotated = rotated_image.eval(feed_dict={image: adv, angle: theta})\n",
    "    p_naive.append(probs.eval(feed_dict={image: rotated})[0][demo_target])\n",
    "\n",
    "robust_line, = plt.plot(thetas, p_robust, color='b', linewidth=2, label='robust')\n",
    "naive_line, = plt.plot(thetas, p_naive, color='r', linewidth=2, label='naive')\n",
    "plt.ylim([0, 1.05])\n",
    "plt.xlabel('rotation angle')\n",
    "plt.ylabel('target class probability')\n",
    "plt.legend(handles=[robust_line, naive_line], loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
